# Spark Structured Streaming Example


## Steps to Create Environment

* Deploy the ARM Template, which creates...
  * Databricks Workspace, Managed Resource Group for Databricks Workspace
  * Event Hub, Event Hub Namespace, Consumer Group
  * Storage Account, Storage Container
  * Key Vault, Two Secrets (Event Hub connection string, Storage Connection String), and an Access Policy for a given ObjectId (e.g. your own personal object id).
* Collect the output of the ARM template for connection strings and service names.
* Follow the instructions for creating a [Key Vault Backed Secret Scope in Databricks](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)
  * The scope name is "myproject" in the demo.
  * This will create a service principal whose Object ID can be found using: `((Get-AzKeyVault -VaultName sparkdevopsakv).AccessPolicies | Where-Object {$_.DisplayName -match "AzureDatabricks*"}).ObjectID`.
  * Consider adding this object id to your deployment script to **ensure your key vault does not delete the Access Policy allowing Databricks to reach Key vault**.
* From your Azure Databricks workspace,
  * Generate a Personal Access Token and note the URL of your workspace.
  * Execute the `notebooks/mounts.scala` notebook with the appropriate storage information about your storage account.
    * You will have to import or copy the code to your workspace.  Alternatively, run the DevOps pipeline to import the notebook automatically.
* Create an Azure DevOps project for this repo and choose the `devops/azure-pipelines.yml` file as your pipeline.
* Add the following variables:
  * databrickstoken: The Personal Access Token generated from Databricks.
  * resourceConnection: The name of the subscription you will be deploying to.
  * subscriptionId: The subscription Id of the subscription you will be deploying to.
  * userObjectForKV: The Object ID you'd like to pass into your Key Vault so that you may access the keys via the portal.
  * workspaceURL: The URL of your Databricks Workspace (e.g. https://adb-12345689.12.azuredatabricks.net/)
* Run the build pipeline to deploy your job
* Send some data via the Python script `python/send-to-eventhub.py`
  * You'll need to create a virtual environment with the requirements file.
  * You'll need to have the following environment variables:
    * EHUB_SEND_CONN_STR (the event hub's connection string with a Send access policy, generated by ARM template)
    * EHUB_SEND_NAME (the event hub name, default is textehub)
* Observe data being added to your storage account!


## Appendix

### References

* [Spark Eventhub Connector Docs](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md)

### AZ CLI Deployment

```az login
az account set --subscription YOUR_SUBSCRIPTION
az group create --location eastus --name sparkdevops

# Validate the template
az group deployment validate -g sparkdevops --template-file ./arm-templates/azure-deploy.json --parameters @./arm-templates/azuredeploy.parameters.json

# Deploy the template
az group deployment create -g sparkdevops --template-file ./arm-templates/azure-deploy.json --parameters @./arm-templates/azuredeploy.parameters.json --name clideploy
```

### Adding Secrets to Non-Key Vault Backed Scope

```databricks secrets create-scope --scope myproject
databricks secrets create-scope --scope myproject --initial-manage-principal users
databricks secrets put --scope myproject --key ehublistenconnstr
databricks secrets put --scope myproject --key storagekey```